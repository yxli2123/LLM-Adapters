description: qlora_gsm8k

target:
  service: aml
  # name: tscience-a100-80g-eastus
  name: A100-80G-PCIE-westus3
  # name: V10032G
  # name: A100EastUS
  # name: openai-A10080G
  # name: A10080G
  # name: gpu-v100-32g
  # name: gpu-a100-80g


environment:
  image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
  image_setup:
    - apt-get -y update
    - apt-get -y install wget
    - apt-get -y install git
  setup:
    - pip install transformers
    - pip install accelerate
    - pip install evaluate scikit-learn scipy typing_extensions einops
    - pip install datasets sentencepiece setuptools rouge-score nltk openai
    - pip install tensorboard tensorboardX
    - pip install fire gradio black appdirs wandb

storage:
  output:
    storage_account_name: tsinterns
    container_name: t-qingru
    mount_dir: /mnt/t-qingru

code:
  local_dir: ../

jobs:
- name: qlora_gsm8k
  sku: 1xG4
  process_count_per_node: 1
  submit_args:
    container_args:
      cpus: 32
  command:
    - CUDA_VISIBLE_DEVICES=0 torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:10 --nnodes=1 --nproc-per-node=1 finetune_qlora.py --num_bits 4 --num_iter 1 --reduced_rank 8 --base_model meta-llama/Llama-2-7b-hf --data_path math_data.json --output_dir /mnt/t-qingru/exp_results/gsm8k/ --batch_size 4 --micro_batch_size 4 --num_epochs 3 --learning_rate 3e-4 --cutoff_len 256 --val_set_size 120 &
    - CUDA_VISIBLE_DEVICES=1 torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:20 --nnodes=1 --nproc-per-node=1 finetune_qlora.py --num_bits 4 --num_iter 1 --reduced_rank 16 --base_model meta-llama/Llama-2-7b-hf --data_path math_data.json --output_dir /mnt/t-qingru/exp_results/gsm8k/ --batch_size 4 --micro_batch_size 4 --num_epochs 3 --learning_rate 3e-4 --cutoff_len 256 --val_set_size 120 &
    - CUDA_VISIBLE_DEVICES=2 torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:30 --nnodes=1 --nproc-per-node=1 finetune_qlora.py --num_bits 4 --num_iter 0 --reduced_rank 8 --base_model meta-llama/Llama-2-7b-hf --data_path math_data.json --output_dir /mnt/t-qingru/exp_results/gsm8k/ --batch_size 4 --micro_batch_size 4 --num_epochs 3 --learning_rate 3e-4 --cutoff_len 256 --val_set_size 120 &
    - sleep 1200
    - CUDA_VISIBLE_DEVICES=3 torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:40 --nnodes=1 --nproc-per-node=1 finetune_qlora.py --num_bits 4 --num_iter 0 --reduced_rank 16 --base_model meta-llama/Llama-2-7b-hf --data_path math_data.json --output_dir /mnt/t-qingru/exp_results/gsm8k/ --batch_size 4 --micro_batch_size 4 --num_epochs 3 --learning_rate 3e-4 --cutoff_len 256 --val_set_size 120